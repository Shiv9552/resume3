---
---

@string{aps = {American Physical Society,}}

@inproceedings{siddhantnsdi,
  author = {Ray, Siddhant and Jiang, Xi and Guo, Zhuohan and Jiang, Junchen and Feamster, Nick},
  title = {Transformer-based Predictions for Sudden Network Changes (Poster)},
  year = {2024},
  isbn = {9781450398992},
  publisher = {USENIX Association},
  abstract = {Accurate predictions on sudden changes in network states are crucial for the integrity of real-time applications. Traditional heuristic models fall short, especially in tail cases, struggling to capture long-term network dependencies. Modelling network traces as time series sequences, we explore the use of a Transformer model architecture, known for its success in time series prediction, to model network trace dependencies, focusing on sudden change predictions. Our preliminary result on using a Transformer model for predicting one-way delay (OWD) shows observable improvement over the heuristic baseline in prediction loss. This suggests a promising direction for enhancing network predictability and optimizing resource utilization.},
  booktitle = {21st USENIX Symposium on Networked Systems Design and Implementation},
  keywords = {packet-level modeling, transformer},
  location = {Santa Clara, CA},
  series = {NSDI '24},
  selected = {true},
  bibtex_show={true},
  abbr={NSDI24},
  pdf={transformer_nsdi_poster.pdf},
}

@inproceedings{dietmuller2022new,
  author = {Dietm\"{u}ller, Alexander and Ray, Siddhant and Jacob, Romain and Vanbever, Laurent},
  title = {A New Hope for Network Model Generalization},
  year = {2022},
  isbn = {9781450398992},
  publisher = {Association for Computing Machinery},
  address = {New York, NY, USA},
  url = {https://doi.org/10.1145/3563766.3564104},
  doi = {10.1145/3563766.3564104},
  abstract = {Generalizing machine learning (ML) models for network traffic dynamics tends to be considered a lost cause. Hence for every new task, we design new models and train them on model-specific datasets closely mimicking the deployment environments. Yet, an ML architecture called Transformer has enabled previously unimaginable generalization in other domains. Nowadays, one can download a model pre-trained on massive datasets and only fine-tune it for a specific task and context with comparatively little time and data. These fine-tuned models are now state-of-the-art for many benchmarks.We believe this progress could translate to networking and propose a Network Traffic Transformer (NTT), a transformer adapted to learn network dynamics from packet traces. Our initial results are promising: NTT seems able to generalize to new prediction tasks and environments. This study suggests there is still hope for generalization through future research.},
  booktitle = {Proceedings of the 21st ACM Workshop on Hot Topics in Networks},
  pages = {152–159},
  numpages = {8},
  keywords = {packet-level modeling, transformer},
  location = {Austin, Texas},
  series = {HotNets '22},
  selected={true},
  abstract={Generalizing machine learning (ML) models for network traffic dynamics tends to be considered a lost cause. Hence, for every new task, we often resolve to design new models and train them on model-specific datasets collected, whenever possible, in an environment mimicking the model's deployment. This approach essentially gives up on generalization. Yet, an ML architecture called_Transformer_ has enabled previously unimaginable generalization in other domains. Nowadays, one can download a model pre-trained on massive datasets and only fine-tune it for a specific task and context with comparatively little time and data. These fine-tuned models are now state-of-the-art for many benchmarks. We believe this progress could translate to networking and propose a Network Traffic Transformer (NTT), a transformer adapted to learn network dynamics from packet traces. Our initial results are promising: NTT seems able to generalize to new prediction tasks and contexts. This study suggests there is still hope for generalization, though it calls for a lot of future research.},
  code={https://github.com/Siddhant-Ray/Network-Traffic-Transformer/tree/ACM-HotNets},
  arxiv={2207.05843},
  bibtex_show={true},
  abbr={HotNets22},
}

@article{ray2020machine,
  title={Machine learning based cell association for mMTC 5G communication networks},
  bibtex_show={true},
  abstract={With the advent of 5G communication networks, the number of devices on the core 5G network significantly increases. A 5G network is a cloud native, massively connected IoT platform with a huge number of devices hosted on the network as compared to prior generation networks. Previously known Machine Type Communication (MTC), it is now known as massive Machine Type Communication (mMTC) and plays a pivotal role in the new network scenario with a larger pool of devices. As ultra-low latency is the key metric in developing 5G communication, a proper cell association scheme is now required to meet the load and traffic needs of the new network, as compared to the earlier cell association schemes which were based only on the Reference Signal Received Power (RSRP). The eNodeB with the highest RSRP may not always be optimal for cell association to provide the lowest latency. This paper proposes an unsupervised machine learning algorithm, namely Hidden Markov Model (HMM) learning on the network’s telemetry data, which is used to learn network parameters and select the best eNodeB for cell association, with the objective of ultimate ultralow latency. The proposed model uses an HMM learning followed by decoding for selecting the optimal cell for association.},
  author={Ray, Siddhant and Bhattacharyya, Budhaditya},
  journal={International Journal of Mobile Network Design and Innovation},
  volume={10},
  number={1},
  pages={10--16},
  year={2020},
  publisher={Inderscience Publishers (IEL)},
  code={https://github.com/Siddhant-Ray/Machine-Learning-Based-Cell-Association},
  abbr={IJMNDI},
}
